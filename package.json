{
  "name": "aipersuasion-org",
  "version": "1.0.0",
  "description": "AI Persuasion Benchmark - Testing how AI models respond to manipulative system prompts",
  "author": "Joshua Ledbetter <ledbetterljoshua@gmail.com>",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/ledbetterljoshua/aipersuasion.org.git"
  },
  "homepage": "https://aipersuasion.org",
  "private": false,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "eslint"
  },
  "dependencies": {
    "react": "19.1.0",
    "react-dom": "19.1.0",
    "next": "15.5.6"
  },
  "devDependencies": {
    "typescript": "^5",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "@tailwindcss/postcss": "^4",
    "tailwindcss": "^4",
    "eslint": "^9",
    "eslint-config-next": "15.5.6",
    "@eslint/eslintrc": "^3"
  }
}
